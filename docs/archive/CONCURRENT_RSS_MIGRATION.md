# RSSå¹¶å‘æŠ“å–ä¼˜åŒ– - è¿ç§»æŒ‡å—

## ğŸ“Š æ€§èƒ½æå‡

- **é¢„æœŸæ€§èƒ½æå‡**: 60-80%
- **ä¸²è¡Œè€—æ—¶**: ~86ç§’ï¼ˆ86ä¸ªæº Ã— 1ç§’/æºï¼‰
- **å¹¶å‘è€—æ—¶**: ~15ç§’ï¼ˆ86ä¸ªæº / 10å¹¶å‘ Ã— 1.5ç§’/æ‰¹æ¬¡ï¼‰
- **åŠ é€Ÿæ¯”**: çº¦5-6å€

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…æ–°ä¾èµ–

```bash
pip install aiohttp>=3.8.0
# æˆ–
pip install -r requirements.txt
```

### 2. ä½¿ç”¨å¹¶å‘ç‰ˆæœ¬ï¼ˆæ¨èï¼‰

#### æ–¹å¼A: ä½¿ç”¨æ–°çš„SearchPipelineV2ï¼ˆæ¨èï¼‰

```python
from search.search_pipeline_v2 import SearchPipelineV2

# åˆ›å»ºç®¡é“ï¼ˆé»˜è®¤ä½¿ç”¨å¹¶å‘æ¨¡å¼ï¼‰
pipeline = SearchPipelineV2(
    hours=24,
    max_items_per_source=20,
    use_concurrent=True,  # ä½¿ç”¨å¹¶å‘ï¼ˆé»˜è®¤ï¼‰
    max_concurrent=10     # æœ€å¤§å¹¶å‘æ•°
)

# è¿è¡Œå®Œæ•´ç®¡é“
news, stats = pipeline.run_pipeline()

# æˆ–ä»…è¿è¡Œæœç´¢
news, stats = pipeline.search_recent_ai_news()
```

#### æ–¹å¼B: ç›´æ¥ä½¿ç”¨ConcurrentRSSFetcher

```python
from search.concurrent_rss_fetcher import ConcurrentRSSFetcher

# åˆ›å»ºå¹¶å‘æŠ“å–å™¨
fetcher = ConcurrentRSSFetcher(
    hours=24,
    max_items_per_source=20,
    max_concurrent=10,  # å¹¶å‘æ•°
    timeout=15,         # è¶…æ—¶ï¼ˆç§’ï¼‰
    max_retries=2       # é‡è¯•æ¬¡æ•°
)

# åŒæ­¥è°ƒç”¨
news, stats = fetcher.fetch_rss_sync()

# å¼‚æ­¥è°ƒç”¨
import asyncio
news, stats = await fetcher.fetch_all_rss_concurrent()
```

### 3. å‘åå…¼å®¹ï¼šä¿æŒä¸²è¡Œæ¨¡å¼

```python
from search.search_pipeline_v2 import SearchPipelineV2

# ä½¿ç”¨ä¸²è¡Œæ¨¡å¼
pipeline = SearchPipelineV2(
    hours=24,
    use_concurrent=False  # å…³é—­å¹¶å‘
)

news, stats = pipeline.run_pipeline()
```

## ğŸ“ ä¿®æ”¹ç°æœ‰ä»£ç 

### ä¿®æ”¹main.py

**åŸä»£ç **:
```python
from search import SearchPipeline

pipeline = SearchPipeline(hours=hours)
raw_news, search_stats = pipeline.search_recent_ai_news()
```

**æ–°ä»£ç **ï¼ˆæ¨èï¼‰:
```python
from search.search_pipeline_v2 import SearchPipelineV2

# ä½¿ç”¨å¹¶å‘æ¨¡å¼
pipeline = SearchPipelineV2(
    hours=hours,
    use_concurrent=True,
    max_concurrent=10
)
raw_news, search_stats = pipeline.search_recent_ai_news()
```

**æˆ–ä¿æŒåŸä»£ç ä¸å˜**ï¼ˆSearchPipelineä»å¯ç”¨ï¼‰:
```python
from search import SearchPipeline  # åŸæœ‰ä»£ç æ— éœ€ä¿®æ”¹

pipeline = SearchPipeline(hours=hours)
raw_news, search_stats = pipeline.search_recent_ai_news()
```

## ğŸ”§ é…ç½®å‚æ•°

### ConcurrentRSSFetcherå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| hours | int | 24 | æœç´¢æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ |
| max_items_per_source | int | 20 | æ¯ä¸ªæºæœ€å¤§æ¡æ•° |
| max_concurrent | int | 10 | æœ€å¤§å¹¶å‘æ•° |
| timeout | int | 15 | å•ä¸ªè¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ |
| max_retries | int | 2 | æœ€å¤§é‡è¯•æ¬¡æ•° |

### å¹¶å‘æ•°è°ƒä¼˜å»ºè®®

- **ä½é…ç½®æœºå™¨**: max_concurrent=5
- **æ ‡å‡†é…ç½®**: max_concurrent=10ï¼ˆæ¨èï¼‰
- **é«˜é…ç½®æœºå™¨**: max_concurrent=15-20
- **æ³¨æ„**: å¹¶å‘æ•°è¿‡é«˜å¯èƒ½è§¦å‘æŸäº›RSSæºçš„é™æµ

## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡

å¹¶å‘ç‰ˆæœ¬ä¼šåœ¨statsä¸­è¿”å›é¢å¤–çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ï¼š

```python
news, stats = fetcher.fetch_rss_sync()

# æ€§èƒ½ç»Ÿè®¡
perf = stats['performance']
print(f"æ€»è€—æ—¶: {perf['total_time']:.2f}s")
print(f"å¹³å‡æ¯æº: {perf['avg_time_per_source']:.2f}s")
print(f"æˆåŠŸ/å¤±è´¥: {perf['successful_fetches']}/{perf['failed_fetches']}")
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•

```bash
# æ–¹å¼1: ç›´æ¥è¿è¡Œå¹¶å‘æŠ“å–å™¨
cd src/search
python concurrent_rss_fetcher.py

# æ–¹å¼2: è¿è¡ŒSearchPipelineV2æµ‹è¯•
python search_pipeline_v2.py
```

### é¢„æœŸè¾“å‡º

```
æ€§èƒ½å¯¹æ¯”ç»“æœ
======================================================================
ä¸²è¡Œæ¨¡å¼è€—æ—¶: 86.34s
å¹¶å‘æ¨¡å¼è€—æ—¶: 15.67s
æ€§èƒ½æå‡: 81.8%
åŠ é€Ÿæ¯”: 5.51x
èŠ‚çœæ—¶é—´: 70.67s
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ç½‘ç»œç¯å¢ƒ

- **éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥**
- æŸäº›RSSæºå¯èƒ½æœ‰è®¿é—®é™åˆ¶æˆ–é™æµ
- å›½å†…è®¿é—®æŸäº›å›½å¤–æºå¯èƒ½è¾ƒæ…¢

### 2. é”™è¯¯å¤„ç†

- å¹¶å‘ç‰ˆæœ¬ä¼šè‡ªåŠ¨å¤„ç†å•ä¸ªæºçš„å¤±è´¥ï¼Œä¸å½±å“å…¶ä»–æº
- å¤±è´¥çš„æºä¼šåœ¨statsä¸­æ ‡è®°ä¸º"error_sources"
- è¶…æ—¶çš„è¯·æ±‚ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆæ ¹æ®max_retriesé…ç½®ï¼‰

### 3. å†…å­˜ä½¿ç”¨

- å¹¶å‘æŠ“å–ä¼šåŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œå†…å­˜å ç”¨ç•¥é«˜
- å¯¹äº86ä¸ªæºï¼Œmax_concurrent=10æ—¶ï¼Œé¢å¤–å†…å­˜å ç”¨çº¦20-50MB

### 4. å…¼å®¹æ€§

- **Pythonç‰ˆæœ¬**: >=3.7ï¼ˆéœ€è¦asyncioæ”¯æŒï¼‰
- **ä¾èµ–åŒ…**: éœ€è¦å®‰è£…aiohttp>=3.8.0
- **å‘åå…¼å®¹**: åŸæœ‰SearchPipelineä»å¯æ­£å¸¸ä½¿ç”¨

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
pipeline = SearchPipelineV2(
    hours=24,
    max_items_per_source=20,
    use_concurrent=True,
    max_concurrent=10  # æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´
)
```

### 2. ç›‘æ§å’Œæ—¥å¿—

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO)

# è¿è¡Œç®¡é“
pipeline = SearchPipelineV2(use_concurrent=True)
news, stats = pipeline.run_pipeline()

# æ£€æŸ¥æ€§èƒ½ç»Ÿè®¡
if 'performance' in stats['search']:
    perf = stats['search']['performance']
    if perf['total_time'] > 30:
        logger.warning(f"RSSæŠ“å–è€—æ—¶è¿‡é•¿: {perf['total_time']:.2f}s")
```

### 3. é”™è¯¯å¤„ç†

```python
try:
    pipeline = SearchPipelineV2(use_concurrent=True)
    news, stats = pipeline.run_pipeline()

    # æ£€æŸ¥å¤±è´¥çš„æº
    failed = stats['search']['source_classification']['error_sources']
    if failed:
        logger.warning(f"ä»¥ä¸‹æºæŠ“å–å¤±è´¥: {failed}")

except Exception as e:
    logger.error(f"RSSæŠ“å–å¤±è´¥: {e}")
    # é™çº§åˆ°ä¸²è¡Œæ¨¡å¼
    pipeline = SearchPipelineV2(use_concurrent=False)
    news, stats = pipeline.run_pipeline()
```

## ğŸ”„ å›æ»šæ–¹æ¡ˆ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿå›æ»šåˆ°ä¸²è¡Œæ¨¡å¼ï¼š

### æ–¹æ³•1: ä¿®æ”¹é…ç½®
```python
pipeline = SearchPipelineV2(use_concurrent=False)
```

### æ–¹æ³•2: ä½¿ç”¨åŸå§‹SearchPipeline
```python
from search import SearchPipeline
pipeline = SearchPipeline(hours=24)
```

## ğŸ“ é—®é¢˜åé¦ˆ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. Pythonç‰ˆæœ¬
2. é”™è¯¯æ—¥å¿—
3. ç½‘ç»œç¯å¢ƒï¼ˆæ˜¯å¦ä½¿ç”¨ä»£ç†ï¼‰
4. statsç»Ÿè®¡ä¿¡æ¯

---

**æœ€åæ›´æ–°**: 2026-01-22
**ç‰ˆæœ¬**: v1.0
