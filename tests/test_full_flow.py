#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨æµç¨‹æµ‹è¯•è„šæœ¬

ä» normalize_news å¼€å§‹ï¼Œæ¨¡æ‹Ÿ 200 æ¡ RSS æœç´¢æ•°æ®ï¼Œèµ°å®Œæ‰€æœ‰æµç¨‹ã€‚
Mock æ—¥æœŸè®¾ä¸º 2026-06-06 ä»¥ä¾¿åŒºåˆ†æµ‹è¯•æ•°æ®ã€‚

ç‰ˆæœ¬: v2 - æ›´æ–° mock æ•°æ®ä»¥éªŒè¯ update æµç¨‹
"""

import json
import logging
import os
import random
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
SRC_DIR = os.path.join(PROJECT_ROOT, 'src')
if SRC_DIR not in sys.path:
    sys.path.insert(0, SRC_DIR)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================
# Mock æ•°æ®ç”Ÿæˆå™¨
# ============================================

# AI æŠ•èµ„æ–°é—»çš„ä¸»é¢˜æ¨¡æ¿
NEWS_TEMPLATES = [
    # èèµ„ç±»
    {
        "title_template": "{company} å®Œæˆ {amount} ç¾å…ƒ {round} è½®èèµ„",
        "content_template": "{company} å®£å¸ƒå®Œæˆ {amount} ç¾å…ƒ {round} è½®èèµ„ï¼Œç”± {investor} é¢†æŠ•ã€‚æ­¤æ¬¡èèµ„å°†ç”¨äº {purpose}ã€‚å…¬å¸ä¼°å€¼è¾¾åˆ° {valuation} ç¾å…ƒã€‚",
        "category": "funding",
        "signal": "Positive"
    },
    {
        "title_template": "{company} è·å¾— {amount} ç¾å…ƒæŠ•èµ„ï¼ŒåŠ é€Ÿ AI å•†ä¸šåŒ–",
        "content_template": "{company} ä»Šæ—¥å®£å¸ƒè·å¾— {amount} ç¾å…ƒæŠ•èµ„ï¼ŒæŠ•èµ„æ–¹ä¸º {investor}ã€‚å…¬å¸ CEO è¡¨ç¤ºå°†åˆ©ç”¨è¿™ç¬”èµ„é‡‘åŠ é€Ÿ {product} çš„å•†ä¸šåŒ–è¿›ç¨‹ï¼Œé¢„è®¡å¹´åº•å‰å°†å®ç° {target}ã€‚",
        "category": "funding",
        "signal": "Positive"
    },
    # äº§å“å‘å¸ƒç±»
    {
        "title_template": "{company} å‘å¸ƒå…¨æ–° {product}ï¼Œæ€§èƒ½æå‡ {percent}%",
        "content_template": "{company} æ­£å¼å‘å¸ƒæœ€æ–°ä¸€ä»£ {product}ï¼Œç›¸æ¯”ä¸Šä¸€ä»£äº§å“æ€§èƒ½æå‡ {percent}%ã€‚æ–°äº§å“é‡‡ç”¨ {technology} æŠ€æœ¯ï¼Œå°†åœ¨ {region} å¸‚åœºç‡å…ˆä¸Šå¸‚ï¼Œå”®ä»· {price} ç¾å…ƒã€‚",
        "category": "product",
        "signal": "Positive"
    },
    {
        "title_template": "{company} æ¨å‡ºä¼ä¸šçº§ AI {product}ï¼Œç„å‡† {market} å¸‚åœº",
        "content_template": "{company} å®£å¸ƒæ¨å‡ºé¢å‘ä¼ä¸šçš„ AI {product}ï¼Œä¸»æ‰“ {feature} åŠŸèƒ½ã€‚è¯¥äº§å“å·²ä¸ {partner} è¾¾æˆåˆä½œï¼Œé¢„è®¡å°†è¦†ç›– {coverage} å®¶ä¼ä¸šå®¢æˆ·ã€‚",
        "category": "product",
        "signal": "Positive"
    },
    # èŠ¯ç‰‡ç¡¬ä»¶ç±»
    {
        "title_template": "{company} æ–°æ¬¾ {chip} èŠ¯ç‰‡å¼€å§‹é‡äº§ï¼Œç®—åŠ›æå‡ {percent}%",
        "content_template": "{company} å®£å¸ƒå…¶æ–°æ¬¾ {chip} AI èŠ¯ç‰‡å·²å¼€å§‹é‡äº§ï¼Œç›¸æ¯”å‰ä»£äº§å“ç®—åŠ›æå‡ {percent}%ï¼Œèƒ½æ•ˆæ¯”æå‡ {efficiency}%ã€‚é¦–æ‰¹èŠ¯ç‰‡å°†ä¾›åº”ç»™ {customer}ã€‚",
        "category": "chip",
        "signal": "Positive"
    },
    {
        "title_template": "GPU ä¾›åº”ç´§å¼ ï¼š{company} é¢„è®¡ {chip} äº¤è´§å‘¨æœŸå»¶é•¿è‡³ {weeks} å‘¨",
        "content_template": "ç”±äº AI éœ€æ±‚æ¿€å¢ï¼Œ{company} è¡¨ç¤ºå…¶ {chip} ç³»åˆ—äº§å“çš„äº¤è´§å‘¨æœŸå·²å»¶é•¿è‡³ {weeks} å‘¨ã€‚åˆ†æå¸ˆé¢„è®¡è¿™ä¸€æƒ…å†µå°†æŒç»­åˆ° {quarter} å­£åº¦æœ«ã€‚",
        "category": "chip",
        "signal": "Neutral"
    },
    # æ”¶è´­åˆå¹¶ç±»
    {
        "title_template": "{acquirer} å®£å¸ƒä»¥ {amount} ç¾å…ƒæ”¶è´­ AI åˆåˆ›å…¬å¸ {target}",
        "content_template": "{acquirer} ä»Šæ—¥å®£å¸ƒå°†ä»¥ {amount} ç¾å…ƒæ”¶è´­ AI åˆåˆ›å…¬å¸ {target}ã€‚æ­¤æ¬¡æ”¶è´­å°†å¸®åŠ© {acquirer} å¢å¼ºåœ¨ {field} é¢†åŸŸçš„æŠ€æœ¯èƒ½åŠ›ã€‚äº¤æ˜“é¢„è®¡äº {quarter} å­£åº¦å®Œæˆã€‚",
        "category": "acquisition",
        "signal": "Positive"
    },
    {
        "title_template": "{company} ä¸ {partner} è¾¾æˆæˆ˜ç•¥åˆä½œï¼Œå…±åŒå¼€å‘ {product}",
        "content_template": "{company} ä¸ {partner} å®£å¸ƒè¾¾æˆæˆ˜ç•¥åˆä½œåè®®ï¼ŒåŒæ–¹å°†å…±åŒå¼€å‘ {product}ã€‚åˆä½œæ¶‰åŠæŠ€æœ¯ã€å¸‚åœºå’Œä¾›åº”é“¾ç­‰å¤šä¸ªå±‚é¢ï¼Œé¢„è®¡æŠ•å…¥ {amount} ç¾å…ƒã€‚",
        "category": "partnership",
        "signal": "Positive"
    },
    # è´¢æŠ¥ä¸šç»©ç±»
    {
        "title_template": "{company} Q{quarter} è´¢æŠ¥è¶…é¢„æœŸï¼ŒAI æ”¶å…¥å¢é•¿ {percent}%",
        "content_template": "{company} å…¬å¸ƒ Q{quarter} è´¢æŠ¥ï¼Œè¥æ”¶ {revenue} ç¾å…ƒï¼ŒåŒæ¯”å¢é•¿ {growth}%ï¼Œå…¶ä¸­ AI ç›¸å…³ä¸šåŠ¡æ”¶å…¥å¢é•¿ {percent}%ã€‚å…¬å¸ä¸Šè°ƒå…¨å¹´ä¸šç»©æŒ‡å¼•è‡³ {guidance} ç¾å…ƒã€‚",
        "category": "earnings",
        "signal": "Positive"
    },
    {
        "title_template": "{company} AI ä¸šåŠ¡æ‰¿å‹ï¼ŒQ{quarter} åˆ©æ¶¦ä¸‹æ»‘ {percent}%",
        "content_template": "{company} å‘å¸ƒ Q{quarter} è´¢æŠ¥æ˜¾ç¤ºï¼Œå—å¸‚åœºç«äº‰åŠ å‰§å½±å“ï¼Œå…¬å¸å‡€åˆ©æ¶¦åŒæ¯”ä¸‹æ»‘ {percent}%ã€‚ç®¡ç†å±‚è¡¨ç¤ºå°†åŠ å¤§ AI ç ”å‘æŠ•å…¥ï¼Œé¢„è®¡ä¸‹å­£åº¦æƒ…å†µå°†æœ‰æ‰€æ”¹å–„ã€‚",
        "category": "earnings",
        "signal": "Risk"
    },
    # ç›‘ç®¡æ”¿ç­–ç±»
    {
        "title_template": "{region} å‡ºå° AI æ–°è§„ï¼š{regulation}",
        "content_template": "{region} æ”¿åºœä»Šæ—¥å‘å¸ƒ AI ç›‘ç®¡æ–°è§„ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬ {regulation}ã€‚æ–°è§„å°†äº {date} èµ·æ­£å¼å®æ–½ï¼Œé¢„è®¡å°†å½±å“ {company} ç­‰å…¬å¸åœ¨è¯¥åœ°åŒºçš„ä¸šåŠ¡ã€‚",
        "category": "regulation",
        "signal": "Risk"
    },
    {
        "title_template": "{region} åŠ å¤§ AI èŠ¯ç‰‡å‡ºå£ç®¡åˆ¶ï¼Œ{company} å—å½±å“",
        "content_template": "{region} å®£å¸ƒè¿›ä¸€æ­¥æ”¶ç´§ AI èŠ¯ç‰‡å‡ºå£ç®¡åˆ¶æªæ–½ï¼Œé™åˆ¶å‘ {target_region} å‡ºå£å…ˆè¿› AI èŠ¯ç‰‡ã€‚{company} è¡¨ç¤ºæ­£åœ¨è¯„ä¼°æ–°è§„å¯¹ä¸šåŠ¡çš„å½±å“ã€‚",
        "category": "regulation",
        "signal": "Risk"
    },
    # æŠ€æœ¯çªç ´ç±»
    {
        "title_template": "{company} å‘å¸ƒ {model} æ¨¡å‹ï¼Œåœ¨ {benchmark} ä¸Šåˆ›æ–°çºªå½•",
        "content_template": "{company} å‘å¸ƒæœ€æ–° AI æ¨¡å‹ {model}ï¼Œåœ¨ {benchmark} åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ° {score} åˆ†ï¼Œåˆ·æ–°è¡Œä¸šçºªå½•ã€‚è¯¥æ¨¡å‹å‚æ•°é‡ä¸º {params}ï¼Œè®­ç»ƒæˆæœ¬çº¦ {cost} ç¾å…ƒã€‚",
        "category": "research",
        "signal": "Positive"
    },
    {
        "title_template": "{company} å¼€æº {model}ï¼Œæ¨åŠ¨ AI æ°‘ä¸»åŒ–",
        "content_template": "{company} å®£å¸ƒå¼€æºå…¶ {model} æ¨¡å‹ï¼Œå…è®¸å¼€å‘è€…å…è´¹ä½¿ç”¨å’Œä¿®æ”¹ã€‚è¯¥æ¨¡å‹åœ¨ {benchmark} æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå·²è·å¾— {downloads} æ¬¡ä¸‹è½½ã€‚",
        "category": "research",
        "signal": "Positive"
    },
    # è¡Œä¸šåŠ¨æ€ç±»
    {
        "title_template": "AI è¡Œä¸šæŠ¥å‘Šï¼š{year} å¹´å¸‚åœºè§„æ¨¡å°†è¾¾ {amount} ç¾å…ƒ",
        "content_template": "æ ¹æ®æœ€æ–°è¡Œä¸šæŠ¥å‘Šï¼Œå…¨çƒ AI å¸‚åœºè§„æ¨¡é¢„è®¡åœ¨ {year} å¹´è¾¾åˆ° {amount} ç¾å…ƒï¼Œå¹´å¤åˆå¢é•¿ç‡ä¸º {cagr}%ã€‚{segment} é¢†åŸŸå¢é•¿æœ€å¿«ï¼Œé¢„è®¡å æ¯”å°†è¾¾ {percent}%ã€‚",
        "category": "industry",
        "signal": "Neutral"
    },
    {
        "title_template": "{company} CEOï¼šAI å°†åœ¨ {years} å¹´å†…æ”¹å˜ {industry}",
        "content_template": "{company} CEO {name} åœ¨æœ€æ–°é‡‡è®¿ä¸­è¡¨ç¤ºï¼ŒAI æŠ€æœ¯å°†åœ¨ {years} å¹´å†…å½»åº•æ”¹å˜ {industry} è¡Œä¸šçš„è¿è¥æ–¹å¼ã€‚ä»–é¢„è®¡å…¬å¸åœ¨è¯¥é¢†åŸŸçš„æŠ•å…¥å°†è¾¾åˆ° {amount} ç¾å…ƒã€‚",
        "category": "industry",
        "signal": "Neutral"
    },
    # äººäº‹å˜åŠ¨ç±»
    {
        "title_template": "{company} ä»»å‘½ {name} ä¸ºé¦–å¸­ AI å®˜",
        "content_template": "{company} å®£å¸ƒä»»å‘½å‰ {prev_company} é«˜ç®¡ {name} ä¸ºå…¬å¸é¦–å¸­ AI å®˜ã€‚{name} å°†è´Ÿè´£é¢†å¯¼å…¬å¸çš„ AI æˆ˜ç•¥å’ŒæŠ€æœ¯ç ”å‘å·¥ä½œã€‚",
        "category": "personnel",
        "signal": "Neutral"
    },
    # å¸‚åœºç«äº‰ç±»
    {
        "title_template": "{company} ä¸ {competitor} å±•å¼€ AI {product} ä»·æ ¼æˆ˜",
        "content_template": "{company} å®£å¸ƒå°†å…¶ AI {product} ä»·æ ¼ä¸‹è°ƒ {percent}%ï¼Œæ­¤ä¸¾è¢«è§†ä¸ºå¯¹ {competitor} è¿‘æœŸé™ä»·çš„å›åº”ã€‚åˆ†æå¸ˆè®¤ä¸ºä»·æ ¼æˆ˜å°†åŠ é€Ÿè¡Œä¸šæ•´åˆã€‚",
        "category": "competition",
        "signal": "Risk"
    },
    # æ•°æ®ä¸­å¿ƒåŸºç¡€è®¾æ–½
    {
        "title_template": "{company} æŠ•èµ„ {amount} ç¾å…ƒå»ºè®¾ AI æ•°æ®ä¸­å¿ƒ",
        "content_template": "{company} å®£å¸ƒå°†åœ¨ {region} æŠ•èµ„ {amount} ç¾å…ƒå»ºè®¾æ–°çš„ AI æ•°æ®ä¸­å¿ƒã€‚è¯¥æ•°æ®ä¸­å¿ƒé¢„è®¡äº {year} å¹´æŠ•å…¥è¿è¥ï¼Œå°†é…å¤‡ {gpus} å— GPUã€‚",
        "category": "infrastructure",
        "signal": "Positive"
    },
    # åº”ç”¨è½åœ°ç±»
    {
        "title_template": "{company} AI åŠ©æ‰‹æ—¥æ´»ç”¨æˆ·çªç ´ {users} ä¸‡",
        "content_template": "{company} å…¬å¸ƒå…¶ AI åŠ©æ‰‹äº§å“æ•°æ®ï¼Œæ—¥æ´»è·ƒç”¨æˆ·å·²çªç ´ {users} ä¸‡ï¼Œæœˆæ´»ç”¨æˆ·è¾¾åˆ° {mau} ä¸‡ã€‚ç”¨æˆ·å¹³å‡ä½¿ç”¨æ—¶é•¿ä¸º {minutes} åˆ†é’Ÿã€‚",
        "category": "application",
        "signal": "Positive"
    },
]

# å…¬å¸åˆ—è¡¨ (v2 æ›´æ–°: æ–°å¢æ›´å¤šå…¬å¸)
COMPANIES = [
    "OpenAI", "Google DeepMind", "Anthropic", "Microsoft", "Meta", "NVIDIA",
    "AMD", "Intel", "Apple", "Amazon", "Alibaba", "ByteDance", "Baidu",
    "Tencent", "Huawei", "xAI", "Mistral AI", "Cohere", "Stability AI",
    "Midjourney", "Runway", "Inflection AI", "Character.AI", "Adept AI",
    "Scale AI", "Databricks", "Snowflake", "Palantir", "C3.ai", "SambaNova",
    "Cerebras", "Groq", "Tesla", "IBM", "Oracle", "Salesforce", "Adobe",
    "Qualcomm", "Broadcom", "TSMC", "Samsung", "SK Hynix",
    # v2 æ–°å¢å…¬å¸
    "DeepSeek", "Zhipu AI", "Moonshot AI", "Minimax", "01.AI", "Perplexity",
    "Reka AI", "Together AI", "Anyscale", "Hugging Face", "Lightmatter"
]

# æŠ•èµ„æœºæ„
INVESTORS = [
    "çº¢æ‰èµ„æœ¬", "Andreessen Horowitz", "è½¯é“¶æ„¿æ™¯åŸºé‡‘", "Tiger Global",
    "Benchmark", "Accel", "General Catalyst", "Lightspeed Venture Partners",
    "Index Ventures", "Founders Fund", "Khosla Ventures", "GV",
    "Microsoft Ventures", "Google Ventures", "NVIDIA Ventures"
]

# æ–°é—»æ¥æº
NEWS_SOURCES = [
    "TechCrunch", "VentureBeat AI", "The Verge", "Wired", "Ars Technica",
    "Bloomberg Technology", "Reuters Technology", "CNBC Technology",
    "The Information", "MIT Technology Review", "IEEE Spectrum",
    "Synced Review", "Analytics India Magazine", "MarkTechPost",
    "SemiAnalysis", "Tom's Hardware", "EE Times", "Hacker News"
]

# äº§å“åç§°
PRODUCTS = [
    "GPT-5", "Gemini Ultra", "Claude 4", "Llama 4", "Stable Diffusion 4",
    "H200 GPU", "MI400 AI èŠ¯ç‰‡", "Gaudi 3", "ç¥ç»ç½‘ç»œå¤„ç†å™¨",
    "ä¼ä¸šç‰ˆ Copilot", "AI ä»£ç åŠ©æ‰‹", "æ™ºèƒ½å®¢æœç³»ç»Ÿ", "è‡ªåŠ¨é©¾é©¶å¥—ä»¶",
    "AI è§†é¢‘ç”Ÿæˆå™¨", "å¤šæ¨¡æ€ AI å¹³å°", "AI æœç´¢å¼•æ“", "æ™ºèƒ½åˆ†æå¹³å°"
]

# åœ°åŒº
REGIONS = ["ç¾å›½", "æ¬§ç›Ÿ", "ä¸­å›½", "æ—¥æœ¬", "éŸ©å›½", "å°åº¦", "æ–°åŠ å¡", "è‹±å›½"]


def generate_mock_content(template: Dict[str, Any], index: int) -> Dict[str, Any]:
    """
    æ ¹æ®æ¨¡æ¿ç”Ÿæˆ mock æ–°é—»å†…å®¹

    Args:
        template: æ–°é—»æ¨¡æ¿
        index: æ–°é—»ç´¢å¼•

    Returns:
        Dict: mock æ–°é—»æ•°æ®
    """
    company = random.choice(COMPANIES)
    company2 = random.choice([c for c in COMPANIES if c != company])
    investor = random.choice(INVESTORS)
    source = random.choice(NEWS_SOURCES)
    product = random.choice(PRODUCTS)
    region = random.choice(REGIONS)

    # éšæœºæ•°æ®
    amount = random.choice(["5000ä¸‡", "1äº¿", "2.5äº¿", "5äº¿", "10äº¿", "20äº¿", "50äº¿", "100äº¿"])
    round_name = random.choice(["A", "B", "C", "D", "E", "Pre-IPO"])
    percent = random.randint(15, 200)
    quarter = random.randint(1, 4)
    weeks = random.randint(12, 36)
    year = random.randint(2025, 2030)
    users = random.randint(100, 5000)
    gpus = random.randint(10000, 100000)

    # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    title = template["title_template"].format(
        company=company, amount=amount, round=round_name,
        percent=percent, product=product, region=region,
        chip=random.choice(["H200", "B200", "MI400", "Gaudi 3"]),
        acquirer=company, target=company2, partner=company2,
        quarter=quarter, weeks=weeks, year=year,
        model=random.choice(["GPT-5", "Gemini 2", "Claude 4", "Llama 4"]),
        benchmark=random.choice(["MMLU", "HumanEval", "GSM8K", "HellaSwag"]),
        market=random.choice(["åŒ»ç–—", "é‡‘è", "æ•™è‚²", "åˆ¶é€ ä¸š"]),
        competitor=company2, users=users, name=f"John_{index}",
        regulation=random.choice(["æ•°æ®å®‰å…¨è¦æ±‚", "ç®—æ³•é€æ˜åº¦è¦æ±‚", "AI ä¼¦ç†å‡†åˆ™"]),
        industry=random.choice(["åŒ»ç–—", "é‡‘è", "åˆ¶é€ ", "é›¶å”®", "ç‰©æµ"]),
        years=random.randint(3, 10)
    )

    content = template["content_template"].format(
        company=company, amount=amount, round=round_name,
        investor=investor, purpose=random.choice(["æŠ€æœ¯ç ”å‘", "å¸‚åœºæ‹“å±•", "äººæ‰æ‹›è˜", "åŸºç¡€è®¾æ–½å»ºè®¾"]),
        valuation=random.choice(["10äº¿", "50äº¿", "100äº¿", "500äº¿"]),
        product=product, percent=percent, technology=random.choice(["Transformer", "æ‰©æ•£æ¨¡å‹", "å¤šæ¨¡æ€", "å¼ºåŒ–å­¦ä¹ "]),
        region=region, price=random.randint(100, 10000),
        feature=random.choice(["æ™ºèƒ½åˆ†æ", "è‡ªåŠ¨åŒ–å¤„ç†", "å®æ—¶ç›‘æ§", "é¢„æµ‹å»ºæ¨¡"]),
        partner=company2, coverage=random.randint(100, 10000),
        chip=random.choice(["H200", "B200", "MI400", "Gaudi 3"]),
        efficiency=random.randint(20, 80),
        customer=random.choice(["Microsoft", "Google", "Amazon", "Meta"]),
        weeks=weeks, quarter=quarter,
        acquirer=company, target=company2,
        field=random.choice(["è®¡ç®—æœºè§†è§‰", "è‡ªç„¶è¯­è¨€å¤„ç†", "æ¨èç³»ç»Ÿ", "æœºå™¨äºº"]),
        revenue=random.choice(["100äº¿", "200äº¿", "500äº¿", "1000äº¿"]),
        growth=random.randint(10, 100),
        guidance=random.choice(["500äº¿", "800äº¿", "1200äº¿"]),
        regulation=random.choice(["æ•°æ®æœ¬åœ°åŒ–å­˜å‚¨", "ç®—æ³•å¤‡æ¡ˆ", "å†…å®¹å®¡æ ¸"]),
        date=f"2025å¹´{random.randint(1, 12)}æœˆ{random.randint(1, 28)}æ—¥",
        target_region=random.choice(["ä¸­å›½", "ä¿„ç½—æ–¯", "ä¸­ä¸œ"]),
        model=random.choice(["GPT-5", "Gemini 2", "Claude 4", "Llama 4"]),
        benchmark=random.choice(["MMLU", "HumanEval", "GSM8K"]),
        score=random.randint(85, 99),
        params=random.choice(["1ä¸‡äº¿", "5000äº¿", "2000äº¿"]),
        cost=random.choice(["5000ä¸‡", "1äº¿", "5äº¿"]),
        downloads=random.randint(10000, 1000000),
        cagr=random.randint(15, 45),
        segment=random.choice(["ç”Ÿæˆå¼AI", "è®¡ç®—æœºè§†è§‰", "NLP", "æœºå™¨å­¦ä¹ å¹³å°"]),
        name=f"CEO_{index}",
        prev_company=company2,
        competitor=company2,
        year=year,
        gpus=gpus,
        users=users,
        mau=users * random.randint(3, 10),
        minutes=random.randint(5, 60),
        years=random.randint(3, 10),
        industry=random.choice(["åŒ»ç–—", "é‡‘è", "åˆ¶é€ ", "é›¶å”®"])
    )

    # ä½¿ç”¨å›ºå®šçš„ Mock æ—¥æœŸ: 2026-06-06
    base_date = datetime(2026, 6, 6)
    random_days = random.randint(0, 7)
    news_date = base_date - timedelta(days=random_days)

    return {
        "title": f"[Mock-v2-{index}] {title}",
        "content": content,
        "source": source,
        "date": news_date.strftime("%Y-%m-%d"),
        "url": f"https://mock-news-v2.example.com/article/{index}",
        "published_at": news_date.isoformat(),
        "mock_category": template["category"],
        "mock_signal": template["signal"]
    }


def generate_mock_rss_data(count: int = 200) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆ mock RSS æœç´¢æ•°æ®

    Args:
        count: ç”Ÿæˆçš„æ–°é—»æ•°é‡

    Returns:
        List[Dict]: mock æ–°é—»åˆ—è¡¨
    """
    # v2: ä½¿ç”¨ä¸åŒçš„éšæœºç§å­ç”Ÿæˆä¸åŒçš„æ•°æ®
    random.seed(20260606)
    logger.info(f"å¼€å§‹ç”Ÿæˆ {count} æ¡ mock RSS æ•°æ® (v2 ç‰ˆæœ¬)...")

    news_list = []
    for i in range(1, count + 1):
        template = random.choice(NEWS_TEMPLATES)
        news = generate_mock_content(template, i)
        news_list.append(news)

    logger.info(f"ç”Ÿæˆå®Œæˆï¼Œå…± {len(news_list)} æ¡æ–°é—»")

    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
    category_stats = {}
    for news in news_list:
        cat = news.get("mock_category", "unknown")
        category_stats[cat] = category_stats.get(cat, 0) + 1

    logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {category_stats}")

    return news_list


def mock_fetch_article(news: Dict[str, Any]) -> Dict[str, Any]:
    """
    Mock åŸæ–‡æŠ“å–ï¼šç”Ÿæˆæ›´è¯¦ç»†çš„å†…å®¹

    Args:
        news: æ–°é—»æ•°æ®

    Returns:
        Dict: æ·»åŠ äº†æŠ“å–å†…å®¹çš„æ–°é—»
    """
    # ç”Ÿæˆæ›´é•¿çš„å†…å®¹ï¼ˆæ¨¡æ‹Ÿç½‘é¡µæŠ“å–ï¼‰
    fetched_content = news["content"] + "\n\n"

    # æ·»åŠ æ›´å¤šç»†èŠ‚
    fetched_content += "ã€è¯¦ç»†æŠ¥é“ã€‘\n"
    fetched_content += f"æœ¬æŠ¥é“æ¥æºäº {news['source']}ï¼Œå‘å¸ƒæ—¶é—´ä¸º {news['date']}ã€‚\n\n"

    # æ·»åŠ ä¸€äº›æ•°å­—å’Œå¼•ç”¨
    fetched_content += f"æ®æ‚‰ï¼Œæ­¤æ¬¡äº‹ä»¶æ¶‰åŠé‡‘é¢çº¦ ${random.randint(100, 1000)}ä¸‡ç¾å…ƒã€‚"
    fetched_content += "è¡Œä¸šåˆ†æå¸ˆè¡¨ç¤ºï¼š\"è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ï¼Œå°†å¯¹è¡Œä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚\"\n\n"

    # æ·»åŠ æ›´å¤šèƒŒæ™¯ä¿¡æ¯
    fetched_content += f"èƒŒæ™¯ä¿¡æ¯ï¼šè¯¥å…¬å¸æˆç«‹äº {random.randint(2010, 2023)} å¹´ï¼Œ"
    fetched_content += f"ç›®å‰å‘˜å·¥è§„æ¨¡çº¦ {random.randint(100, 10000)} äººï¼Œ"
    fetched_content += f"å¹´è¥æ”¶çº¦ ${random.randint(10, 500)} äº¿ç¾å…ƒã€‚"

    news["fetched_content"] = fetched_content
    news["fetched_title"] = news["title"]
    news["fetch_stats"] = {
        "status_code": 200,
        "content_length": len(fetched_content),
        "fetch_time_ms": random.randint(100, 2000)
    }

    return news


def run_full_flow_test():
    """
    è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
    """
    from search.search_result_process import normalize_news, SearchResultProcessor
    from selector.news_selector import NewsSelectorPipeline
    from selector.selector_config import TOP_K_SELECT
    from event.event_pipeline import EventPipeline
    from event.decision import EventDecisionPipeline
    from content import ArticleBuilder, MarkdownRenderer
    from webapp_exporter import export_to_webapp

    logger.info("=" * 60)
    logger.info("å¼€å§‹å…¨æµç¨‹æµ‹è¯•ï¼ˆä» normalize_news å¼€å§‹ï¼‰")
    logger.info("=" * 60)

    stats = {}

    # ============================================
    # ç¬¬é›¶æ­¥ï¼šç”Ÿæˆ 200 æ¡ Mock RSS æ•°æ®
    # ============================================
    logger.info("\nç¬¬é›¶æ­¥ï¼šç”Ÿæˆ 200 æ¡ Mock RSS æ•°æ®...")
    raw_news = generate_mock_rss_data(200)
    stats["mock_data_count"] = len(raw_news)

    # ============================================
    # ç¬¬äºŒæ­¥ï¼šè§„èŒƒåŒ–æ–°é—»ï¼ˆä»è¿™é‡Œå¼€å§‹æ˜¯æ­£å¼æµç¨‹ï¼‰
    # ============================================
    logger.info("\nç¬¬äºŒæ­¥ï¼šè§„èŒƒåŒ–æ–°é—»æ•°æ®...")
    news_list, normalize_stats = normalize_news(raw_news, max_items=200)
    logger.info(f"è§„èŒƒåŒ–å®Œæˆï¼Œå…± {len(news_list)} æ¡æœ‰æ•ˆæ–°é—»")
    stats["normalize_stats"] = normalize_stats

    if not news_list:
        logger.error("è§„èŒƒåŒ–åæ— æœ‰æ•ˆæ–°é—»ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return None, stats

    # ============================================
    # ç¬¬ä¸‰æ­¥ï¼šåŸæ–‡æŠ“å–ï¼ˆMockï¼‰
    # ============================================
    logger.info("\nç¬¬ä¸‰æ­¥ï¼šåŸæ–‡æŠ“å–ï¼ˆMockï¼‰...")
    fetch_stats = {"total": 0, "success": 0, "failed": 0}

    for news in news_list:
        fetch_stats["total"] += 1
        try:
            mock_fetch_article(news)
            fetch_stats["success"] += 1
        except Exception as e:
            fetch_stats["failed"] += 1
            logger.error(f"Mock æŠ“å–å¤±è´¥: {e}")

    stats["fetch_stats"] = fetch_stats
    logger.info(f"Mock æŠ“å–å®Œæˆ: æˆåŠŸ {fetch_stats['success']}/{fetch_stats['total']}")

    # ============================================
    # ç¬¬å››æ­¥ï¼šæµç¨‹å¤„ç†ï¼ˆå»é‡â†’åˆå¹¶ï¼‰
    # ============================================
    logger.info("\nç¬¬å››æ­¥ï¼šå»é‡ä¸åˆå¹¶...")
    processor = SearchResultProcessor(similarity_threshold=0.6)
    processed_news, pipeline_stats = processor.process_search_results(news_list)
    logger.info(f"å¤„ç†å®Œæˆï¼Œè¾“å‡º {len(processed_news)} æ¡æ–°é—»")
    stats["pipeline_stats"] = pipeline_stats

    if not processed_news:
        logger.error("å¤„ç†åæ— æœ‰æ•ˆæ–°é—»ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return None, stats

    # ============================================
    # ç¬¬äº”æ­¥ï¼šè½»é‡åŒ–ç‰¹å¾æŠ½å–ï¼ˆç®€åŒ–ç‰ˆ Mockï¼‰
    # ============================================
    logger.info("\nç¬¬äº”æ­¥ï¼šè½»é‡åŒ–ç‰¹å¾æŠ½å–ï¼ˆMockï¼‰...")
    light_stats = {"total": 0, "success": 0}

    for news in processed_news:
        light_stats["total"] += 1
        content = news.get("fetched_content", news.get("content", ""))

        # Mock è½»é‡åŒ–ç‰¹å¾
        news["light_features"] = {
            "content_length": len(content),
            "has_numbers": bool(random.random() > 0.3),  # 70% æ¦‚ç‡æœ‰æ•°å­—
            "has_quote": bool(random.random() > 0.5),     # 50% æ¦‚ç‡æœ‰å¼•ç”¨
            "company_count": random.randint(1, 5),
            "signal_term_count": random.randint(0, 8)
        }
        light_stats["success"] += 1

    stats["light_features_stats"] = light_stats
    logger.info(f"è½»é‡åŒ–ç‰¹å¾æŠ½å–å®Œæˆ: {light_stats['success']}/{light_stats['total']}")

    # ============================================
    # ç¬¬å…­æ­¥ï¼šæ–°é—»é€‰æ‹©ï¼ˆè¯„åˆ†â†’æ’åºâ†’é€‰æ‹©ï¼‰
    # ============================================
    logger.info("\nç¬¬å…­æ­¥ï¼šæ–°é—»é€‰æ‹©...")
    selector = NewsSelectorPipeline(top_k=TOP_K_SELECT)
    final_news, select_stats = selector.select_news(processed_news)
    logger.info(f"é€‰æ‹©å®Œæˆï¼Œè¾“å‡º {len(final_news)} æ¡æ–°é—»")
    stats["select_stats"] = select_stats

    if not final_news:
        logger.error("é€‰æ‹©åæ— æœ‰æ•ˆæ–°é—»ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return None, stats

    # ============================================
    # ç¬¬ä¸ƒæ­¥ï¼šæŠ•èµ„ä¿¡æ¯æŠ½å–ï¼ˆMock LLM ç»“æœï¼‰
    # ============================================
    logger.info("\nç¬¬ä¸ƒæ­¥ï¼šæŠ•èµ„ä¿¡æ¯æŠ½å–ï¼ˆMock LLMï¼‰...")
    extract_stats = {"total": 0, "success": 0}

    for news in final_news:
        extract_stats["total"] += 1

        # Mock æŠ•èµ„ä¿¡æ¯
        news["investment_info"] = {
            "core_thesis": f"å…³äº {news.get('title', '')[:30]} çš„æŠ•èµ„åˆ†æ",
            "market_impact": random.choice(["ç§¯æå½±å“", "ä¸­æ€§å½±å“", "éœ€è¦è§‚å¯Ÿ"]),
            "risk_factors": [
                random.choice(["å¸‚åœºç«äº‰", "æŠ€æœ¯é£é™©", "ç›‘ç®¡é£é™©", "æ‰§è¡Œé£é™©"]),
                random.choice(["ä¼°å€¼è¿‡é«˜", "ç›ˆåˆ©èƒ½åŠ›", "ç°é‡‘æµ", "äººæ‰æµå¤±"])
            ],
            "time_horizon": random.choice(["çŸ­æœŸ", "ä¸­æœŸ", "é•¿æœŸ"]),
            "related_tickers": random.sample(["NVDA", "MSFT", "GOOGL", "META", "AMZN", "AMD", "INTC"], k=random.randint(1, 3)),
            "confidence_level": random.choice(["é«˜", "ä¸­", "ä½"])
        }
        news["ai_summary"] = f"è¿™æ˜¯ä¸€æ¡å…³äº AI è¡Œä¸šçš„é‡è¦æ–°é—»ã€‚{news['content'][:100]}..."
        extract_stats["success"] += 1

    stats["investment_extract_stats"] = extract_stats
    logger.info(f"æŠ•èµ„ä¿¡æ¯æŠ½å–å®Œæˆ: {extract_stats['success']}/{extract_stats['total']}")

    # ============================================
    # ç¬¬å…«æ­¥ï¼šäº‹ä»¶åˆ†æï¼ˆåµŒå…¥â†’èšç±»â†’æ‘˜è¦ï¼‰
    # ============================================
    logger.info("\nç¬¬å…«æ­¥ï¼šäº‹ä»¶åˆ†æ...")
    try:
        event_pipeline = EventPipeline()
        events, event_stats = event_pipeline.analyze_events(final_news)
        logger.info(f"äº‹ä»¶åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {len(events)} ä¸ªäº‹ä»¶")
        stats["event_stats"] = event_stats
    except Exception as e:
        logger.error(f"äº‹ä»¶åˆ†æå¤±è´¥: {e}", exc_info=True)
        # å¦‚æœäº‹ä»¶åˆ†æå¤±è´¥ï¼Œåˆ›å»º mock äº‹ä»¶
        events = []
        for i in range(min(5, len(final_news))):
            events.append({
                "representative_title": final_news[i].get("title", f"Mock äº‹ä»¶ {i + 1}"),
                "summary": final_news[i].get("ai_summary", "è¿™æ˜¯ä¸€ä¸ª Mock äº‹ä»¶æ‘˜è¦"),
                "news_count": random.randint(1, 5),
                "sources": [final_news[i].get("source", "Mock Source")],
                "companies": random.sample(COMPANIES[:10], k=random.randint(1, 3)),
                "news_indices": [i]
            })
        stats["event_stats"] = {"mock": True, "events_count": len(events)}

    # ============================================
    # ç¬¬ä¹æ­¥ï¼šäº‹ä»¶å†³ç­–
    # ============================================
    logger.info("\nç¬¬ä¹æ­¥ï¼šäº‹ä»¶å†³ç­–...")
    try:
        decision_pipeline = EventDecisionPipeline()
        events_with_decision, decision_stats = decision_pipeline.decide_with_stats(events)
        logger.info(f"äº‹ä»¶å†³ç­–å®Œæˆï¼Œä¸º {len(events_with_decision)} ä¸ªäº‹ä»¶ç”Ÿæˆå†³ç­–")
        stats["decision_stats"] = decision_stats
        events = events_with_decision
    except Exception as e:
        logger.error(f"äº‹ä»¶å†³ç­–å¤±è´¥: {e}", exc_info=True)
        # Mock å†³ç­–
        for event in events:
            event["decision"] = {
                "importance": random.choice(["High", "Medium", "Low"]),
                "signal": random.choice(["Positive", "Neutral", "Risk"]),
                "action": random.choice(["Watch", "Hold", "Avoid"])
            }
        stats["decision_stats"] = {"mock": True}

    # ============================================
    # ç¬¬åæ­¥ï¼šå…¬ä¼—å·æ–‡ç« ç”Ÿæˆ
    # ============================================
    logger.info("\nç¬¬åæ­¥ï¼šå…¬ä¼—å·æ–‡ç« ç”Ÿæˆ...")
    try:
        article_builder = ArticleBuilder()
        article = article_builder.build(events)

        renderer = MarkdownRenderer()
        article_content = renderer.render(article)

        # ä¿å­˜æ–‡ç« 
        output_dir = os.path.join(PROJECT_ROOT, "output")
        os.makedirs(output_dir, exist_ok=True)

        # ä½¿ç”¨å›ºå®šçš„ Mock æ—¥æœŸ
        date_str = "20260606"
        filename = f"mock_ai_invest_article_{date_str}.md"
        file_path = os.path.join(output_dir, filename)

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(article_content)

        logger.info(f"å…¬ä¼—å·æ–‡ç« å·²ä¿å­˜åˆ°: {file_path}")
        stats["article_stats"] = {"success": True, "file_path": file_path}
    except Exception as e:
        logger.error(f"å…¬ä¼—å·æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
        stats["article_stats"] = {"error": str(e)}

    # ============================================
    # ç¬¬åä¸€æ­¥ï¼šH5 åº”ç”¨æ•°æ®å¯¼å‡º
    # ============================================
    logger.info("\nç¬¬åä¸€æ­¥ï¼šH5 åº”ç”¨æ•°æ®å¯¼å‡º...")

    # æ„å»ºç»“æœæ•°æ®ï¼ˆä½¿ç”¨å›ºå®šçš„ Mock æ—¥æœŸï¼‰
    mock_date = "2026-06-06"
    result = {
        "date": mock_date,
        "news": final_news,
        "events": events
    }

    try:
        export_result = export_to_webapp(result, stats)

        if export_result.get("success"):
            logger.info("H5 åº”ç”¨æ•°æ®å¯¼å‡ºæˆåŠŸ!")
            logger.info(f"  æ•°æ®æ–‡ä»¶: {export_result.get('data_file')}")
            logger.info(f"  ç´¢å¼•æ–‡ä»¶: {export_result.get('index_file')}")
            stats["webapp_export_stats"] = {
                "success": True,
                "data_file": export_result.get("data_file"),
                "index_file": export_result.get("index_file")
            }
        else:
            logger.error(f"H5 åº”ç”¨æ•°æ®å¯¼å‡ºå¤±è´¥: {export_result.get('error')}")
            stats["webapp_export_stats"] = {"success": False, "error": export_result.get("error")}
    except Exception as e:
        logger.error(f"H5 åº”ç”¨æ•°æ®å¯¼å‡ºå¼‚å¸¸: {e}", exc_info=True)
        stats["webapp_export_stats"] = {"error": str(e)}

    return result, stats


def main():
    """ä¸»å‡½æ•°"""
    try:
        result, stats = run_full_flow_test()

        print("\n" + "=" * 60)
        print("å…¨æµç¨‹æµ‹è¯•å®Œæˆ")
        print("=" * 60)

        if result:
            print(f"\næ—¥æœŸ: {result.get('date')}")
            print(f"æœ€ç»ˆæ–°é—»æ•°é‡: {len(result.get('news', []))}")
            print(f"äº‹ä»¶æ•°é‡: {len(result.get('events', []))}")

            print("\nã€äº‹ä»¶åˆ—è¡¨ã€‘")
            for i, event in enumerate(result.get("events", [])[:5], 1):
                print(f"\näº‹ä»¶ {i}:")
                print(f"  æ ‡é¢˜: {event.get('representative_title', 'N/A')[:60]}...")
                print(f"  æ–°é—»æ•°é‡: {event.get('news_count', 0)}")
                decision = event.get("decision", {})
                if decision:
                    print(f"  å†³ç­–: {decision.get('importance')} | {decision.get('signal')} | {decision.get('action')}")

        print("\nã€æµç¨‹ç»Ÿè®¡ã€‘")
        print(f"Mock æ•°æ®: {stats.get('mock_data_count', 0)} æ¡")
        print(f"è§„èŒƒåŒ–å: {sum(stats.get('normalize_stats', {}).values())} æ¡")

        pipeline_stats = stats.get("pipeline_stats", {})
        if pipeline_stats:
            print(f"å»é‡å: {pipeline_stats.get('step1_dedup', {}).get('kept_count', 0)} æ¡")
            print(f"åˆå¹¶å: {pipeline_stats.get('step2_merge', {}).get('output_count', 0)} æ¡")

        print(f"é€‰æ‹©å: {stats.get('select_stats', {}).get('output_count', 0)} æ¡")
        print(f"äº‹ä»¶æ•°: {stats.get('event_stats', {}).get('valid_events', 0)} ä¸ª")

        webapp_stats = stats.get("webapp_export_stats", {})
        if webapp_stats.get("success"):
            print(f"\nâœ… H5 åº”ç”¨æ•°æ®å·²å¯¼å‡º:")
            print(f"  æ•°æ®æ–‡ä»¶: {webapp_stats.get('data_file')}")
            print(f"  ç´¢å¼•æ–‡ä»¶: {webapp_stats.get('index_file')}")
            print(f"\nğŸŒ è¯·è®¿é—® http://localhost:8080 æŸ¥çœ‹æµ‹è¯•ç»“æœ")

        logger.info("=" * 60)
        logger.info("æµ‹è¯•å®Œæˆ")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
