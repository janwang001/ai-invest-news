#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hacker News é¦–é¡µç›‘æ§

ç›‘æ§HNé¦–é¡µTop Storiesä¸­çš„AIç›¸å…³çƒ­é—¨è®¨è®º:
- ä½¿ç”¨å®˜æ–¹HN API (æ— éœ€è®¤è¯)
- ç­›é€‰AI/MLç›¸å…³å¸–å­
- åˆ†æè¯„è®ºçƒ­åº¦å’Œæƒ…ç»ª

HNé¦–é¡µ = æŠ€æœ¯ç¤¾åŒºé£å‘æ ‡
"""

import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests

logger = logging.getLogger(__name__)


class HackerNewsMonitor:
    """Hacker Newsé¦–é¡µç›‘æ§å™¨"""

    def __init__(self):
        # HNå®˜æ–¹API
        self.api_base = "https://hacker-news.firebaseio.com/v0"

        # AIç›¸å…³å…³é”®è¯
        self.ai_keywords = [
            "ai", "gpt", "llm", "chatgpt", "openai", "anthropic", "claude",
            "gemini", "llama", "mistral", "transformer", "neural", "deep learning",
            "machine learning", "ml", "artificial intelligence", "agi",
            "diffusion", "stable diffusion", "midjourney", "dall-e",
            "embedding", "vector", "rag", "agent", "langchain",
            "nvidia", "cuda", "gpu", "tpu", "inference",
        ]

        # é«˜æƒé‡å…³é”®è¯ï¼ˆæ¶‰åŠå•†ä¸š/æŠ•èµ„ï¼‰
        self.investment_keywords = [
            "funding", "raised", "valuation", "ipo", "acquisition",
            "billion", "million", "revenue", "profit", "layoff",
            "regulation", "antitrust", "ftc", "lawsuit",
            "release", "launch", "announce", "partnership",
        ]

        # çƒ­åº¦é˜ˆå€¼
        self.thresholds = {
            "min_score": 50,           # æœ€å°åˆ†æ•°
            "min_comments": 20,        # æœ€å°è¯„è®ºæ•°
            "hot_score": 200,          # çƒ­é—¨é˜ˆå€¼
            "hot_comments": 100,       # çƒ­é—¨è¯„è®ºæ•°
        }

        self.headers = {
            "User-Agent": "AI-Investment-Monitor/1.0"
        }

    def fetch_top_stories(self, limit: int = 100) -> List[int]:
        """
        è·å–é¦–é¡µTop Storiesçš„IDåˆ—è¡¨

        Args:
            limit: è·å–æ•°é‡

        Returns:
            story IDåˆ—è¡¨
        """
        try:
            url = f"{self.api_base}/topstories.json"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            story_ids = response.json()
            return story_ids[:limit]
        except Exception as e:
            logger.error(f"è·å–HNé¦–é¡µå¤±è´¥: {e}")
            return []

    def fetch_story(self, story_id: int) -> Optional[Dict]:
        """
        è·å–å•ä¸ªstoryè¯¦æƒ…

        Args:
            story_id: story ID

        Returns:
            storyè¯¦æƒ…
        """
        try:
            url = f"{self.api_base}/item/{story_id}.json"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.debug(f"è·å–story {story_id}å¤±è´¥: {e}")
            return None

    def fetch_ai_stories(self, hours: int = 24) -> List[Dict]:
        """
        è·å–AIç›¸å…³çš„çƒ­é—¨stories

        Args:
            hours: æ—¶é—´èŒƒå›´

        Returns:
            ç­›é€‰åçš„storyåˆ—è¡¨
        """
        try:
            # è·å–é¦–é¡µstories
            story_ids = self.fetch_top_stories(limit=100)
            if not story_ids:
                logger.warning("æœªè·å–åˆ°HNé¦–é¡µstories")
                return []

            cutoff_time = datetime.now() - timedelta(hours=hours)
            cutoff_timestamp = cutoff_time.timestamp()

            ai_stories = []

            for story_id in story_ids:
                try:
                    story = self.fetch_story(story_id)
                    if not story:
                        continue

                    # æ£€æŸ¥æ—¶é—´
                    story_time = story.get("time", 0)
                    if story_time < cutoff_timestamp:
                        continue

                    # æ£€æŸ¥æ˜¯å¦AIç›¸å…³
                    title = story.get("title", "").lower()
                    url = story.get("url", "").lower()

                    if not self._is_ai_related(title, url):
                        continue

                    # æ£€æŸ¥çƒ­åº¦
                    score = story.get("score", 0)
                    comments = story.get("descendants", 0)

                    if score < self.thresholds["min_score"]:
                        continue

                    # è®¡ç®—ä¼˜å…ˆçº§
                    priority = self._calculate_priority(score, comments, title)

                    # æå–ä¿¡å·
                    signal = self._extract_signal(title)

                    processed = {
                        "id": story_id,
                        "title": story.get("title", ""),
                        "url": story.get("url", ""),
                        "hn_url": f"https://news.ycombinator.com/item?id={story_id}",
                        "score": score,
                        "comments": comments,
                        "author": story.get("by", ""),
                        "time": datetime.fromtimestamp(story_time).strftime("%Y-%m-%d %H:%M"),
                        "source": "Hacker News",
                        "priority": priority,
                        "investment_signal": signal,
                        "is_hot": score >= self.thresholds["hot_score"] or comments >= self.thresholds["hot_comments"],
                    }

                    ai_stories.append(processed)
                    time.sleep(0.1)  # APIç¤¼è²Œå»¶è¿Ÿ

                except Exception as e:
                    logger.debug(f"å¤„ç†story {story_id}å¤±è´¥: {e}")
                    continue

            # æŒ‰ä¼˜å…ˆçº§å’Œåˆ†æ•°æ’åº
            ai_stories.sort(
                key=lambda x: (
                    0 if x["priority"] == "P0" else 1 if x["priority"] == "P1" else 2,
                    -x["score"]
                )
            )

            logger.info(f"HN: å‘ç° {len(ai_stories)} æ¡AIç›¸å…³çƒ­é—¨è®¨è®º")
            return ai_stories[:20]  # æœ€å¤šè¿”å›20æ¡

        except Exception as e:
            logger.error(f"è·å–HN AI storieså¤±è´¥: {e}")
            return []

    def _is_ai_related(self, title: str, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦AIç›¸å…³"""
        text = f"{title} {url}".lower()

        for keyword in self.ai_keywords:
            if keyword in text:
                return True

        return False

    def _calculate_priority(self, score: int, comments: int, title: str) -> str:
        """è®¡ç®—ä¼˜å…ˆçº§"""
        title_lower = title.lower()

        # P0: è¶…çƒ­é—¨ æˆ– æŠ•èµ„ç›¸å…³å…³é”®è¯
        if score >= 500 or comments >= 300:
            return "P0"

        for keyword in self.investment_keywords:
            if keyword in title_lower:
                if score >= 100:
                    return "P0"

        # P1: çƒ­é—¨
        if score >= self.thresholds["hot_score"] or comments >= self.thresholds["hot_comments"]:
            return "P1"

        # P2: ä¸€èˆ¬
        return "P2"

    def _extract_signal(self, title: str) -> str:
        """æå–æŠ•èµ„ä¿¡å·"""
        title_lower = title.lower()

        # æ­£é¢ä¿¡å·
        positive_keywords = [
            "launch", "release", "announce", "funding", "raised",
            "breakthrough", "milestone", "growth", "success",
        ]

        # è´Ÿé¢ä¿¡å·
        negative_keywords = [
            "layoff", "shut down", "cancel", "fail", "lawsuit",
            "investigation", "breach", "hack", "vulnerable",
        ]

        for kw in positive_keywords:
            if kw in title_lower:
                return "Positive"

        for kw in negative_keywords:
            if kw in title_lower:
                return "Negative"

        return "Neutral"

    def generate_test_data(self) -> List[Dict]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        now = datetime.now()
        return [
            {
                "id": 39001234,
                "title": "OpenAI announces GPT-5 with reasoning capabilities",
                "url": "https://openai.com/blog/gpt5",
                "hn_url": "https://news.ycombinator.com/item?id=39001234",
                "score": 1250,
                "comments": 567,
                "author": "pg",
                "time": now.strftime("%Y-%m-%d %H:%M"),
                "source": "Hacker News",
                "priority": "P0",
                "investment_signal": "Positive",
                "is_hot": True,
            },
            {
                "id": 39001235,
                "title": "Anthropic raises $2B at $15B valuation",
                "url": "https://techcrunch.com/anthropic-funding",
                "hn_url": "https://news.ycombinator.com/item?id=39001235",
                "score": 890,
                "comments": 234,
                "author": "dang",
                "time": (now - timedelta(hours=3)).strftime("%Y-%m-%d %H:%M"),
                "source": "Hacker News",
                "priority": "P0",
                "investment_signal": "Positive",
                "is_hot": True,
            },
            {
                "id": 39001236,
                "title": "Show HN: I built an open-source LLM inference engine",
                "url": "https://github.com/example/llm-engine",
                "hn_url": "https://news.ycombinator.com/item?id=39001236",
                "score": 320,
                "comments": 89,
                "author": "builder",
                "time": (now - timedelta(hours=6)).strftime("%Y-%m-%d %H:%M"),
                "source": "Hacker News",
                "priority": "P1",
                "investment_signal": "Neutral",
                "is_hot": True,
            },
            {
                "id": 39001237,
                "title": "EU AI Act enforcement begins in Q3 2026",
                "url": "https://europa.eu/ai-act-update",
                "hn_url": "https://news.ycombinator.com/item?id=39001237",
                "score": 156,
                "comments": 112,
                "author": "eutech",
                "time": (now - timedelta(hours=12)).strftime("%Y-%m-%d %H:%M"),
                "source": "Hacker News",
                "priority": "P1",
                "investment_signal": "Neutral",
                "is_hot": True,
            },
        ]


def test_hackernews_monitor():
    """æµ‹è¯•HNç›‘æ§å™¨"""
    print("=" * 60)
    print("Hacker News ç›‘æ§å™¨æµ‹è¯•")
    print("=" * 60)

    monitor = HackerNewsMonitor()

    # æµ‹è¯•æ•°æ®
    print("\næµ‹è¯•1: æµ‹è¯•æ•°æ®ç”Ÿæˆ")
    test_stories = monitor.generate_test_data()
    print(f"  ç”Ÿæˆ {len(test_stories)} æ¡æµ‹è¯•stories")

    for story in test_stories:
        print(f"\n  [{story['priority']}] {story['title'][:50]}...")
        print(f"    â¬†ï¸ {story['score']} | ğŸ’¬ {story['comments']} | {'ğŸ”¥' if story['is_hot'] else ''}")
        print(f"    ä¿¡å·: {story['investment_signal']}")

    # æµ‹è¯•AIç›¸å…³åˆ¤æ–­
    print("\næµ‹è¯•2: AIç›¸å…³æ€§åˆ¤æ–­")
    test_cases = [
        ("OpenAI announces GPT-5", "https://openai.com", True),
        ("My weekend project", "https://github.com/me", False),
        ("LLM inference optimization", "https://arxiv.org", True),
        ("New JavaScript framework", "https://js.dev", False),
    ]

    for title, url, expected in test_cases:
        result = monitor._is_ai_related(title, url)
        status = "âœ…" if result == expected else "âŒ"
        print(f"  {status} {title} â†’ {result} (æœŸæœ›: {expected})")

    # æµ‹è¯•çœŸå®APIï¼ˆå¯èƒ½å¤±è´¥ï¼‰
    print("\næµ‹è¯•3: çœŸå®APIè·å–ï¼ˆå¯èƒ½éœ€è¦ç­‰å¾…ï¼‰")
    try:
        stories = monitor.fetch_ai_stories(hours=72)
        print(f"  è·å–åˆ° {len(stories)} æ¡AIç›¸å…³stories")

        for story in stories[:3]:
            print(f"\n    [{story['priority']}] {story['title'][:50]}...")
            print(f"      â¬†ï¸ {story['score']} | ğŸ’¬ {story['comments']}")

    except Exception as e:
        print(f"  APIè·å–å¤±è´¥: {e}")

    print("\næµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    test_hackernews_monitor()
